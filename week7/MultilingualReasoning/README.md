# å¤šè¯­è¨€æ¨ç†æ¨¡å‹å¾®è°ƒ

æœ¬é¡¹ç›®å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Hugging Face çš„ TRL åº“å¯¹ OpenAI çš„å¼€æºæ¨ç†æ¨¡å‹ `openai/gpt-oss-20b` è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿåœ¨å¤šç§è¯­è¨€ä¸­è¿›è¡Œæœ‰æ•ˆæ¨ç†ã€‚

## é¡¹ç›®ç®€ä»‹

å¤§å‹æ¨ç†æ¨¡å‹å¦‚ OpenAI o3 ä¼šç”Ÿæˆæ€ç»´é“¾ï¼ˆchain-of-thoughtï¼‰æ¥æé«˜å“åº”çš„å‡†ç¡®æ€§å’Œè´¨é‡ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°æ¨¡å‹å³ä½¿åœ¨å…¶ä»–è¯­è¨€æé—®æ—¶ä¹Ÿç”¨è‹±è¯­è¿›è¡Œæ¨ç†ã€‚

æœ¬é¡¹ç›®é€šè¿‡ä»¥ä¸‹æ–¹å¼è§£å†³è¿™ä¸ªé—®é¢˜ï¼š
- åœ¨æ¨¡å‹çš„ç³»ç»Ÿæç¤ºä¸­æ·»åŠ "æ¨ç†è¯­è¨€"é€‰é¡¹
- ä½¿ç”¨å¤šè¯­è¨€æ¨ç†æ•°æ®é›†è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰
- æ”¯æŒè‹±è¯­ã€è¥¿ç­ç‰™è¯­ã€æ³•è¯­ã€æ„å¤§åˆ©è¯­ã€å¾·è¯­ç­‰å¤šç§è¯­è¨€çš„æ¨ç†

## åŠŸèƒ½ç‰¹æ€§

âœ¨ **å¤šè¯­è¨€æ¨ç†**ï¼šæ¨¡å‹å¯ä»¥ç”¨å¤šç§è¯­è¨€ç”Ÿæˆæ€ç»´é“¾
ğŸ”€ **æ··åˆè¯­è¨€æ”¯æŒ**ï¼šå¯ä»¥ç”¨ä¸€ç§è¯­è¨€æé—®ï¼Œç”¨å¦ä¸€ç§è¯­è¨€æ¨ç†ï¼Œç”¨ç¬¬ä¸‰ç§è¯­è¨€å›ç­”
ğŸš€ **é«˜æ•ˆè®­ç»ƒ**ï¼šä½¿ç”¨ LoRAï¼ˆä½ç§©é€‚åº”ï¼‰æŠ€æœ¯è¿›è¡Œå†…å­˜é«˜æ•ˆçš„å¾®è°ƒ
ğŸ“Š **å®æ—¶ç›‘æ§**ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­è·Ÿè¸ªæŸå¤±å’ŒæŒ‡æ ‡
ğŸŒ **å¼ºå¤§çš„è·¨è¯­è¨€æ³›åŒ–èƒ½åŠ›**ï¼šè™½ç„¶å¾®è°ƒæ•°æ®é›†ä¸­æ²¡æœ‰ä¸­æ–‡ï¼Œä½†æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ä½¿å…¶èƒ½å¤Ÿç”¨ä¸­æ–‡ç”Ÿæˆæ¨ç†è¿‡ç¨‹ï¼

## ç³»ç»Ÿè¦æ±‚

### âš ï¸ é‡è¦ï¼šæ˜¾å­˜éœ€æ±‚

**è®­ç»ƒè¿‡ç¨‹ä¸­å³°å€¼æ˜¾å­˜å ç”¨çº¦ 97GB**

- **æ¨èé…ç½®**: H200 GPUï¼ˆ141GB æ˜¾å­˜ï¼‰
- **ä¸æ¨è**: å•å¡ 80GB GPUï¼ˆH100/A100ï¼‰- ä¼šå‡ºç° OOMï¼ˆOut of Memoryï¼‰é”™è¯¯
- **æ›¿ä»£æ–¹æ¡ˆ**: 
  - ä½¿ç”¨å¤š GPU è®­ç»ƒï¼ˆæ¨¡å‹å¹¶è¡Œ/æ•°æ®å¹¶è¡Œï¼‰
  - å‡å°æ‰¹æ¬¡å¤§å°ï¼ˆbatch_sizeï¼‰å’Œåºåˆ—é•¿åº¦ï¼ˆmax_seq_lengthï¼‰
  - ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆgradient checkpointingï¼‰
  - ä½¿ç”¨ DeepSpeed ZeRO-3 ç­‰æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯

### å…¶ä»–è¦æ±‚

- CUDA: 12.8 æˆ–æ›´é«˜ç‰ˆæœ¬
- Python: 3.8+
- å­˜å‚¨ç©ºé—´: è‡³å°‘ 100GBï¼ˆç”¨äºæ¨¡å‹å’Œæ£€æŸ¥ç‚¹ï¼‰

## å®‰è£…ä¾èµ–

```bash
# å®‰è£… PyTorchï¼ˆCUDA 12.8ï¼‰
pip install torch --index-url https://download.pytorch.org/whl/cu128

# å®‰è£…å…¶ä»–ä¾èµ–
pip install "trl>=0.20.0" "peft>=0.17.0" "transformers>=4.55.0" trackio datasets accelerate bitsandbytes
```

## å¿«é€Ÿå¼€å§‹

### 1. è®¾ç½®ç¯å¢ƒ

é¦–å…ˆï¼Œç¡®ä¿ä½ å·²ç™»å½• Hugging Faceï¼š

```python
from huggingface_hub import notebook_login
notebook_login()
```

æˆ–ä½¿ç”¨å‘½ä»¤è¡Œï¼š

```bash
huggingface-cli login
```

### 2. å‡†å¤‡æ•°æ®é›†

æœ¬é¡¹ç›®ä½¿ç”¨ `HuggingFaceH4/Multilingual-Thinking` æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†åŒ…å«å¤šç§è¯­è¨€çš„æ¨ç†é“¾ï¼š

```python
from datasets import load_dataset

dataset = load_dataset("HuggingFaceH4/Multilingual-Thinking")
```

**âš ï¸ é‡è¦æç¤º**ï¼šè™½ç„¶è¯¥æ•°æ®é›†ä¸­æ²¡æœ‰åŒ…å«ä¸­æ–‡æ•°æ®ï¼Œä½†å¾—ç›Šäºæ¨¡å‹çš„å¼ºå¤§æ³›åŒ–èƒ½åŠ›ï¼Œå¾®è°ƒåçš„æ¨¡å‹ä¾ç„¶èƒ½å¤Ÿç”¨ä¸­æ–‡è¿›è¡Œæ¨ç†ï¼è¯¦è§ä¸‹æ–¹"å…³äºä¸­æ–‡æ¨ç†çš„é‡è¦è¯´æ˜"ç« èŠ‚ã€‚

### 3. è¿è¡Œå¾®è°ƒ

```bash
python gpt_oss_20b_sft.py
```

### 4. æ¨ç†æµ‹è¯•

å¾®è°ƒå®Œæˆåï¼Œä½ å¯ä»¥ä½¿ç”¨æ¨¡å‹è¿›è¡Œå¤šè¯­è¨€æ¨ç†ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("openai/gpt-oss-20b")
model = AutoModelForCausalLM.from_pretrained("ä½ çš„æ¨¡å‹è·¯å¾„")

# è®¾ç½®æ¨ç†è¯­è¨€
REASONING_LANGUAGE = "German"
SYSTEM_PROMPT = f"reasoning language: {REASONING_LANGUAGE}"
USER_PROMPT = "Â¿CuÃ¡l es el capital de Australia?"  # è¥¿ç­ç‰™è¯­ï¼šæ¾³å¤§åˆ©äºšçš„é¦–éƒ½æ˜¯ä»€ä¹ˆï¼Ÿ

messages = [
    {"role": "system", "content": SYSTEM_PROMPT},
    {"role": "user", "content": USER_PROMPT},
]

# ç”Ÿæˆå“åº”
input_ids = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt")
output_ids = model.generate(input_ids, max_new_tokens=512, temperature=0.6)
response = tokenizer.decode(output_ids[0])
print(response)
```

## è®­ç»ƒé…ç½®

### è¶…å‚æ•°ï¼ˆä¸ OpenAI Cookbook Notebook å®Œå…¨ä¸€è‡´ï¼‰

- **æ¨¡å‹**: `openai/gpt-oss-20b`ï¼ˆ20B å‚æ•°ï¼‰
- **LoRA rank**: 16
- **LoRA alpha**: 16
- **æ‰¹æ¬¡å¤§å°**: 8ï¼ˆper_device_train_batch_sizeï¼‰
- **æ¢¯åº¦ç´¯ç§¯æ­¥æ•°**: 2
- **æœ‰æ•ˆæ‰¹æ¬¡å¤§å°**: 16ï¼ˆ8 Ã— 2ï¼‰
- **å­¦ä¹ ç‡**: 2e-5
- **å­¦ä¹ ç‡è°ƒåº¦å™¨**: cosine
- **é¢„çƒ­æ¯”ä¾‹**: 0.1
- **è®­ç»ƒè½®æ•°**: 3
- **æœ€å¤§åºåˆ—é•¿åº¦**: 2048
- **ä¼˜åŒ–å™¨**: adamw_torch_fused
- **æƒé‡è¡°å‡**: 0.01
- **æ··åˆç²¾åº¦**: bfloat16
- **æ˜¾å­˜å³°å€¼**: ~97GB

### LoRA é…ç½®

ä½¿ç”¨ LoRAï¼ˆLow-Rank Adaptationï¼‰æŠ€æœ¯ï¼Œåªè®­ç»ƒå°‘é‡å‚æ•°ï¼š
- ç›®æ ‡æ¨¡å—ï¼šæŸ¥è¯¢å’Œå€¼æŠ•å½±å±‚ï¼ˆq_proj, v_projï¼‰
- æ˜¾è‘—å‡å°‘è®­ç»ƒæ—¶é—´å’Œå†…å­˜ä½¿ç”¨
- ä¿æŒåŸºç¡€æ¨¡å‹æƒé‡ä¸å˜

## é¡¹ç›®ç»“æ„

```
MultilingualReasoning/
â”œâ”€â”€ README.md                    # é¡¹ç›®æ–‡æ¡£
â”œâ”€â”€ gpt_oss_20b_sft.py          # å®Œæ•´çš„è®­ç»ƒå’Œæ¨ç†è„šæœ¬
â””â”€â”€ requirements.txt             # ä¾èµ–åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
```

## è®­ç»ƒæ—¶é—´å’Œèµ„æºæ¶ˆè€—

### æ ‡å‡†é…ç½®ï¼ˆOpenAI Cookbook æµ‹è¯•ç»“æœï¼‰

- **GPU**: H100ï¼ˆ80GB æ˜¾å­˜ï¼‰
- **è®­ç»ƒæ—¶é—´**: çº¦ 18 åˆ†é’Ÿ
- **å³°å€¼æ˜¾å­˜**: ~97GB âš ï¸ **ä¼šå¯¼è‡´ OOMï¼**

### å®é™…å»ºè®®é…ç½®

ç”±äºå³°å€¼æ˜¾å­˜è¶…è¿‡ 80GBï¼Œå»ºè®®ä½¿ç”¨ï¼š
- **H200 GPU**ï¼ˆ141GB æ˜¾å­˜ï¼‰- å¯ä»¥å®Œç¾è¿è¡Œ
- **å¤š H100 GPU** - ä½¿ç”¨æ¨¡å‹å¹¶è¡Œ
- **ä¼˜åŒ–é…ç½®** - è§ä¸‹æ–¹"æ€§èƒ½ä¼˜åŒ–å»ºè®®"

### å½±å“å› ç´ 

è®­ç»ƒæ—¶é—´å’Œæ˜¾å­˜å ç”¨å–å†³äºï¼š
- GPU å‹å·å’Œæ˜¾å­˜å¤§å°
- æ‰¹æ¬¡å¤§å°ï¼ˆbatch_sizeï¼‰
- åºåˆ—é•¿åº¦ï¼ˆmax_seq_lengthï¼‰
- æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
- æ˜¯å¦ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ

## æ¨ç†ç¤ºä¾‹

```
[ç¤ºä¾‹ 1: è¥¿ç­ç‰™è¯­æé—® + å¾·è¯­æ¨ç†]

ç”Ÿæˆå“åº”...
æ¨ç†è¯­è¨€: German
ç”¨æˆ·æé—®: Â¿CuÃ¡l es el capital de Australia?
--------------------------------------------------------------------------------
<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2025-10-03

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

reasoning language: German

<|end|><|start|>user<|message|>Â¿CuÃ¡l es el capital de Australia?<|end|><|start|>assistant<|channel|>analysis<|message|>Okay, der Benutzer fragt nach der Hauptstadt von Australien. ZunÃ¤chst sollte ich daran denken, dass Australien ein Kontinent und ein Land ist, und seine Hauptstadt ist Canberra. Viele Leute denken fÃ¤lschlicherweise, dass Sydney oder Melbourne die Hauptstadt ist, aber das ist nicht korrekt. Ich sollte sicherstellen, dass ich die richtige Antwort gebe.

Ich frage mich, ob der Benutzer vielleicht nach einer weniger bekannten Information fragt, aber die Frage ist ziemlich direkt. Ich sollte einfach die Hauptstadt nennen. Allerdings kÃ¶nnte der Benutzer nach weiteren Details fragen, wie zum Beispiel, wann Canberra zur Hauptstadt wurde oder warum es nicht Sydney ist. Ich sollte jedoch nur auf die Frage antworten, es sei denn, der Benutzer bittet um weitere Informationen.

Ich sollte auch sicherstellen, dass ich die Antwort klar und prÃ¤zise formuliere. Also, die Hauptstadt von Australien ist Canberra. Ich kann das in einem Satz zusammenfassen. Es ist auch wichtig, die Frage des Benutzers zu berÃ¼cksichtigen und sicherzustellen, dass ich keine Annahmen Ã¼ber sein Vorwissen treffe. Ich sollte nur die grundlegende Information geben, die er angefordert hat.<|end|><|start|>assistant<|channel|>final<|message|>The capital of Australia is Canberra.<|return|>
--------------------------------------------------------------------------------
```

### ğŸŒ å…³äºä¸­æ–‡æ¨ç†çš„é‡è¦è¯´æ˜

**âš ï¸ é›¶æ ·æœ¬æ³›åŒ–èƒ½åŠ›å±•ç¤º**

è™½ç„¶è®­ç»ƒæ•°æ®é›† `HuggingFaceH4/Multilingual-Thinking` ä¸­**æ²¡æœ‰åŒ…å«ä»»ä½•ä¸­æ–‡æ•°æ®**ï¼Œä½†æ¨¡å‹ä¾ç„¶èƒ½å¤Ÿåœ¨ä¸­æ–‡ç¯å¢ƒä¸‹ç”Ÿæˆé«˜è´¨é‡çš„æ¨ç†è¿‡ç¨‹ï¼è¿™æ˜¯å› ä¸ºï¼š

1. **åŸºç¡€æ¨¡å‹çš„å¤šè¯­è¨€èƒ½åŠ›**ï¼šGPT-OSS-20B åŸºç¡€æ¨¡å‹æœ¬èº«æ”¯æŒå¤šç§è¯­è¨€ï¼ŒåŒ…æ‹¬ä¸­æ–‡
2. **æ¨ç†æ¨¡å¼çš„è¿ç§»**ï¼šé€šè¿‡åœ¨å…¶ä»–è¯­è¨€ï¼ˆè‹±è¯­ã€è¥¿ç­ç‰™è¯­ã€å¾·è¯­ç­‰ï¼‰ä¸Šå­¦ä¹ æ¨ç†æ¨¡å¼ï¼Œæ¨¡å‹èƒ½å¤Ÿå°†è¿™ç§èƒ½åŠ›æ³›åŒ–åˆ°æœªè§è¿‡çš„è¯­è¨€
3. **ç³»ç»Ÿæç¤ºçš„å¼•å¯¼ä½œç”¨**ï¼šé€šè¿‡ `reasoning language: Chinese` çš„ç³»ç»Ÿæç¤ºï¼Œæ¨¡å‹èƒ½å¤Ÿç†è§£å¹¶åˆ‡æ¢åˆ°ä¸­æ–‡è¿›è¡Œæ¨ç†

è¿™ç§**é›¶æ ·æœ¬è·¨è¯­è¨€æ³›åŒ–**æ˜¯å¤§å‹è¯­è¨€æ¨¡å‹çš„é‡è¦ç‰¹æ€§ï¼Œå±•ç¤ºäº†å³ä½¿æ²¡æœ‰ç‰¹å®šè¯­è¨€çš„è®­ç»ƒæ•°æ®ï¼Œæ¨¡å‹ä»èƒ½åœ¨æ–°è¯­è¨€ä¸Šè¡¨ç°å‡ºè‰²ã€‚

**å®é™…æ„ä¹‰**ï¼š
- æ— éœ€ä¸ºæ¯ç§è¯­è¨€æ”¶é›†è®­ç»ƒæ•°æ®
- é™ä½äº†å¤šè¯­è¨€æ¨¡å‹çš„è®­ç»ƒæˆæœ¬
- è¯æ˜äº†æ¨ç†èƒ½åŠ›æ˜¯å¯è¿ç§»çš„é€šç”¨èƒ½åŠ›

ä¸‹é¢çš„ç¤ºä¾‹ 2 å’Œç¤ºä¾‹ 3 å±•ç¤ºäº†æ¨¡å‹åœ¨ä¸­æ–‡ç¯å¢ƒä¸‹çš„æ¨ç†èƒ½åŠ›ï¼š

```
[ç¤ºä¾‹ 2: è‹±è¯­æé—® + ä¸­æ–‡æ¨ç†]

ç”Ÿæˆå“åº”...
æ¨ç†è¯­è¨€: Chinese
ç”¨æˆ·æé—®: What is the national symbol of Canada?
--------------------------------------------------------------------------------
<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2025-10-03

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

reasoning language: Chinese

<|end|><|start|>user<|message|>What is the national symbol of Canada?<|end|><|start|>assistant<|channel|>analysis<|message|>
æˆ‘å…ˆæ€è€ƒä¸€ä¸‹åŠ æ‹¿å¤§çš„å›½å®¶è±¡å¾ã€‚åŠ æ‹¿å¤§çš„å›½æ——æ˜¯çº¢ç™½ç›¸é—´çš„ï¼Œä¸­å¤®æœ‰ä¸€ç‰‡æ«å¶ã€‚æ«å¶æ˜¯åŠ æ‹¿å¤§çš„æ ‡å¿—æ€§ç¬¦å·ï¼Œå¸¸è¢«ç”¨æ¥ä»£è¡¨å›½å®¶ã€‚æ«å¶åœ¨åŠ æ‹¿å¤§çš„æ–‡åŒ–å’Œå†å²ä¸­å æœ‰é‡è¦åœ°ä½ï¼Œä¾‹å¦‚æ«ç³–æµ†æ˜¯åŠ æ‹¿å¤§çš„ä¼ ç»Ÿé£Ÿå“ï¼Œæ«å¶ä¹Ÿè¢«ç”¨äºè®¸å¤šå®˜æ–¹å¾½ç« å’Œçºª
  å“ä¸Šã€‚

æˆ‘è¿˜è®°å¾—åŠ æ‹¿å¤§çš„å›½å¾½å’Œå›½æ——éƒ½ä»¥æ«å¶ä¸ºæ ¸å¿ƒå…ƒç´ ã€‚æ«å¶è±¡å¾ç€åŠ æ‹¿å¤§çš„è‡ªç„¶æ™¯è§‚å’Œå¤šæ ·æ€§ï¼Œä¹Ÿä½“ç°äº†åŠ æ‹¿å¤§äººå¯¹åœŸåœ°å’Œè‡ªç„¶çš„çƒ­çˆ±ã€‚å› æ­¤ï¼Œæˆ‘å¯ä»¥è‚¯å®šåœ°è¯´ï¼Œæ«å¶æ˜¯åŠ æ‹¿å¤§çš„å›½å®¶è±¡å¾ã€‚<|end|><|start|>assistant<|channel|>final<|message|>The national symbol of Canada is the **maple leaf**. The maple leaf is prominently featured on Canadaâ€™s national flag (the Red Ensign), its coat of arms, and is widely used in government and cultural contexts to represent Canadian identity. The maple leaf symbolizes Canada's natural environment, its multicultural heritage, and its historical ties to the maple tree, which is abundant in the country.<|return|>
--------------------------------------------------------------------------------
```

```
[ç¤ºä¾‹ 3: ä¸­æ–‡æé—® + ä¸­æ–‡æ¨ç†]

ç”Ÿæˆå“åº”...
æ¨ç†è¯­è¨€: Chinese
ç”¨æˆ·æé—®: æ±‚è§£ x^2 - 2x + 1 = 0 çš„æ ¹
--------------------------------------------------------------------------------
<|start|>system<|message|>You are ChatGPT, a large language model trained by OpenAI.
Knowledge cutoff: 2024-06
Current date: 2025-10-03

Reasoning: medium

# Valid channels: analysis, commentary, final. Channel must be included for every message.<|end|><|start|>developer<|message|># Instructions

reasoning language: Chinese

<|end|><|start|>user<|message|>æ±‚è§£ x^2 - 2x + 1 = 0 çš„æ ¹<|end|><|start|>assistant<|channel|>analysis<|message|>
é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®è®¤æˆ‘ç†è§£äº†ç”¨æˆ·çš„è¾“å…¥ã€‚ç”¨æˆ·è¦æ±‚è§£æ–¹ç¨‹ x^2 - 2x + 1 = 0 çš„æ ¹ã€‚

æ¥ä¸‹æ¥ï¼Œæˆ‘å›æƒ³ä¸€ä¸‹æ±‚è§£äºŒæ¬¡æ–¹ç¨‹çš„ä¸€èˆ¬æ–¹æ³•ã€‚äºŒæ¬¡æ–¹ç¨‹çš„æ ‡å‡†å½¢å¼æ˜¯ ax^2 + bx + c = 0ã€‚è¿™é‡Œï¼Œa = 1ï¼Œb = -2ï¼Œc = 1ã€‚

æˆ‘å¯ä»¥ä½¿ç”¨æ±‚æ ¹å…¬å¼ï¼šx = [-b Â± sqrt(b^2 - 4ac)] / (2a)ã€‚è®©æˆ‘ä»¬ä»£å…¥æ•°å€¼ï¼š

b^2 = (-2)^2 = 4
4ac = 4 * 1 * 1 = 4
b^2 - 4ac = 4 - 4 = 0

ç”±äºåˆ¤åˆ«å¼ (b^2 - 4ac) ä¸º 0ï¼Œæ–¹ç¨‹æœ‰ä¸€ä¸ªé‡æ ¹ã€‚è®¡ç®—æ ¹ï¼š

x = [-b Â± sqrt(0)] / (2a) = [2 Â± 0] / 2 = 2 / 2 = 1

å› æ­¤ï¼Œæ ¹æ˜¯ x = 1ã€‚

æˆ‘æƒ³å†æ£€æŸ¥ä¸€æ¬¡ï¼Œç¡®ä¿æ²¡æœ‰é”™è¯¯ã€‚å°† x = 1 ä»£å…¥åŸæ–¹ç¨‹ï¼š

(1)^2 - 2*(1) + 1 = 1 - 2 + 1 = 0

è¿™éªŒè¯äº†æˆ‘çš„ç­”æ¡ˆã€‚æ ¹æ˜¯ x = 1ï¼Œä¸”ç”±äºåˆ¤åˆ«å¼ä¸º 0ï¼Œåªæœ‰ä¸€ä¸ªé‡æ ¹ã€‚

æˆ‘åº”è¯¥åœ¨å›ç­”ä¸­è¯´æ˜æ ¹æ˜¯ x = 1ï¼Œå¹¶æåˆ°å®ƒæ˜¯ä¸€ä¸ªé‡æ ¹ã€‚
<|end|><|start|>assistant<|channel|>final<|message|>The equation \(x^2 - 2x + 1 = 0\) has a **double root** (also called a repeated root) at \(x = 1\).

**Solution:**

1. **Using the quadratic formula**:
   \[
   x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}
   \]
   For the equation \(x^2 - 2x + 1 = 0\), we have \(a = 1\), \(b = -2\), and \(c = 1\).

   \[
   \Delta = b^2 - 4ac = (-2)^2 - 4 \cdot 1 \cdot 1 = 4 - 4 = 0
   \]

   Since the
--------------------------------------------------------------------------------
```

## æŠ€æœ¯ç»†èŠ‚

### æ•°æ®é›†æ ¼å¼

æ•°æ®é›†ä½¿ç”¨å¯¹è¯æ ¼å¼ï¼ŒåŒ…å«ï¼š
- ç³»ç»Ÿæç¤ºï¼ˆæŒ‡å®šæ¨ç†è¯­è¨€ï¼‰
- ç”¨æˆ·æ¶ˆæ¯ï¼ˆé—®é¢˜ï¼‰
- åŠ©æ‰‹æ¨ç†ï¼ˆæ€ç»´é“¾ï¼‰
- åŠ©æ‰‹å“åº”ï¼ˆæœ€ç»ˆç­”æ¡ˆï¼‰

### LoRA ä¼˜åŠ¿

1. **å†…å­˜æ•ˆç‡**ï¼šåªè®­ç»ƒå°‘é‡å‚æ•°ï¼ˆé€šå¸¸ <1% çš„æ¨¡å‹å‚æ•°ï¼‰
2. **å¿«é€Ÿè®­ç»ƒ**ï¼šå‡å°‘è®¡ç®—éœ€æ±‚
3. **æ˜“äºéƒ¨ç½²**ï¼šå¯ä»¥å°† LoRA æƒé‡ä¸åŸºç¡€æ¨¡å‹åˆå¹¶
4. **å¤šé€‚é…å™¨**ï¼šå¯ä»¥ä¸ºä¸åŒä»»åŠ¡è®­ç»ƒå¤šä¸ª LoRA é€‚é…å™¨

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. å‡å°‘æ˜¾å­˜ä½¿ç”¨ï¼ˆé’ˆå¯¹ 80GB GPUï¼‰

ç”±äºå³°å€¼æ˜¾å­˜éœ€æ±‚ 97GBï¼Œä»¥ä¸‹æ˜¯åœ¨ 80GB GPU ä¸Šè¿è¡Œçš„ä¼˜åŒ–ç­–ç•¥ï¼š

```bash
# æ–¹æ¡ˆ 1: å‡å°æ‰¹æ¬¡å¤§å°
python gpt_oss_20b_sft.py --batch_size 4 --max_seq_length 1536

# æ–¹æ¡ˆ 2: ä½¿ç”¨ 4-bit é‡åŒ–ï¼ˆå¯èƒ½å½±å“ç²¾åº¦ï¼‰
python gpt_oss_20b_sft.py --use_4bit --batch_size 6

# æ–¹æ¡ˆ 3: ç»„åˆä¼˜åŒ–
python gpt_oss_20b_sft.py --batch_size 4 --max_seq_length 1024
```

**æ³¨æ„**: è¿™äº›ä¿®æ”¹ä¼šåç¦»åŸå§‹ notebook é…ç½®ï¼Œå¯èƒ½å½±å“è®­ç»ƒæ•ˆæœã€‚

### 2. ä½¿ç”¨å¤š GPU è®­ç»ƒ

```bash
# ä½¿ç”¨ DeepSpeed ZeRO-3ï¼ˆæ¨èï¼‰
deepspeed --num_gpus=2 gpt_oss_20b_sft.py --mode train

# æˆ–ä½¿ç”¨ PyTorch FSDP
torchrun --nproc_per_node=2 gpt_oss_20b_sft.py --mode train
```

### 3. æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆGradient Checkpointingï¼‰

åœ¨ä»£ç ä¸­å¯ç”¨ï¼ˆä¼šé™ä½è®­ç»ƒé€Ÿåº¦ï¼‰ï¼š
- ä¿®æ”¹è®­ç»ƒå‚æ•°æ·»åŠ : `gradient_checkpointing=True`

### 4. æé«˜æ¨¡å‹è´¨é‡

- å¢åŠ è®­ç»ƒæ•°æ®ï¼ˆæ‰©å±•æ•°æ®é›†ï¼‰
- è°ƒæ•´å­¦ä¹ ç‡ï¼ˆå°è¯• 1e-5 æˆ– 3e-5ï¼‰
- ä½¿ç”¨æ›´å¤§çš„ LoRA rankï¼ˆå¦‚ 32 æˆ– 64ï¼‰

## å¸¸è§é—®é¢˜

**Q: ä¸ºä»€ä¹ˆéœ€è¦ 97GB æ˜¾å­˜ï¼Ÿæˆ‘çš„ H100ï¼ˆ80GBï¼‰å¤Ÿç”¨å—ï¼Ÿ**
A: ä¸å¤Ÿç”¨ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­å³°å€¼æ˜¾å­˜ä¼šè¾¾åˆ° 97GBï¼Œå•å¡ 80GB GPU ä¼šå‡ºç° OOM é”™è¯¯ã€‚å»ºè®®ä½¿ç”¨ H200ï¼ˆ141GBï¼‰æˆ–å¤š GPU è®­ç»ƒã€‚

**Q: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ**
A: æœ‰å‡ ä¸ªé€‰æ‹©ï¼š
   1. ä½¿ç”¨ H200 GPUï¼ˆæ¨èï¼Œå¯å®Œå…¨æŒ‰ notebook é…ç½®è¿è¡Œï¼‰
   2. ä½¿ç”¨å¤šå¡è®­ç»ƒï¼ˆ2 å¼  H100ï¼‰
   3. å‡å°‘æ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•¿åº¦ï¼ˆä¼šåç¦»åŸå§‹é…ç½®ï¼‰
   4. ä½¿ç”¨ DeepSpeed ZeRO-3 æˆ– FSDP

**Q: å¯ä»¥ä½¿ç”¨ A100ï¼ˆ80GBï¼‰è®­ç»ƒå—ï¼Ÿ**
A: ä¸å»ºè®®ã€‚A100 å’Œ H100 éƒ½æ˜¯ 80GB æ˜¾å­˜ï¼Œéƒ½ä¼šé‡åˆ° OOM é—®é¢˜ã€‚

**Q: ä¸ºä»€ä¹ˆ notebook è¯´ H100 å¯ä»¥è¿è¡Œï¼Ÿ**
A: Notebook æ˜¯ç†æƒ³æƒ…å†µçš„ä¼°è®¡ã€‚å®é™…è¿è¡Œæ—¶ç”±äºé¢å¤–çš„å¼€é”€ï¼ˆæ¿€æ´»å€¼ã€ä¼˜åŒ–å™¨çŠ¶æ€ç­‰ï¼‰ï¼Œå³°å€¼æ˜¾å­˜ä¼šè¶…è¿‡ 80GBã€‚

**Q: å¯ä»¥è®­ç»ƒå…¶ä»–è¯­è¨€å—ï¼Ÿ**
A: å¯ä»¥ï¼åªéœ€å‡†å¤‡ç›¸åº”è¯­è¨€çš„æ¨ç†æ•°æ®é›†å³å¯ã€‚

**Q: è®­ç»ƒåå¦‚ä½•è¯„ä¼°æ¨¡å‹ï¼Ÿ**
A: å¯ä»¥åœ¨å¤šè¯­è¨€æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨ç†è´¨é‡ã€å‡†ç¡®æ€§å’Œæµç•…åº¦ã€‚

**Q: ä¿®æ”¹è¶…å‚æ•°åä¼šå½±å“æ•ˆæœå—ï¼Ÿ**
A: ä¼šçš„ã€‚å‡å°æ‰¹æ¬¡å¤§å°ã€åºåˆ—é•¿åº¦ç­‰ä¼šåç¦»åŸå§‹ notebook é…ç½®ï¼Œå¯èƒ½å½±å“æœ€ç»ˆæ¨¡å‹è´¨é‡ã€‚

## å‚è€ƒèµ„æ–™

- [OpenAI GPT-OSS-20B æ¨¡å‹](https://huggingface.co/openai/gpt-oss-20b)
- [Multilingual-Thinking æ•°æ®é›†](https://huggingface.co/datasets/HuggingFaceH4/Multilingual-Thinking)
- [TRL åº“æ–‡æ¡£](https://github.com/huggingface/trl)
- [LoRA è®ºæ–‡](https://arxiv.org/abs/2106.09685)
- [åŸå§‹ Notebook](https://github.com/openai/openai-cookbook/blob/main/articles/gpt-oss/fine-tune-transfomers.ipynb)

## è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ã€æŠ¥å‘Šé—®é¢˜æˆ–æå‡ºæ”¹è¿›å»ºè®®ï¼

## è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªç›¸åº”å¼€æºè®¸å¯è¯ã€‚ä½¿ç”¨æ—¶è¯·éµå®ˆ OpenAI æ¨¡å‹çš„ä½¿ç”¨æ¡æ¬¾ã€‚

## è‡´è°¢

- OpenAI å›¢é˜Ÿå‘å¸ƒçš„å¼€æºæ¨ç†æ¨¡å‹
- Hugging Face å›¢é˜Ÿæä¾›çš„ TRL åº“å’Œæ•°æ®é›†
- åŸå§‹æ•™ç¨‹ä½œè€…ï¼šEdward Beechingã€Quentin GallouÃ©decã€Lewis Tunstall

