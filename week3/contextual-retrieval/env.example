# Contextual Retrieval System Configuration
# Copy this file to .env and fill in your API keys

# LLM Provider Configuration
# Choose your preferred provider for context generation

# Kimi/Moonshot (Recommended for Chinese content)
MOONSHOT_API_KEY=your_kimi_api_key_here

# Doubao
ARK_API_KEY=your_doubao_api_key_here

# OpenAI (Good for general use)
OPENAI_API_KEY=your_openai_api_key_here

# SiliconFlow
SILICONFLOW_API_KEY=your_siliconflow_api_key_here

# OpenRouter
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Groq (Fast inference)
GROQ_API_KEY=your_groq_api_key_here

# Together AI
TOGETHER_API_KEY=your_together_api_key_here

# DeepSeek
DEEPSEEK_API_KEY=your_deepseek_api_key_here

# Default LLM Configuration
LLM_PROVIDER=openai  # Options: kimi, doubao, openai, siliconflow, etc.
LLM_MODEL=gpt-3.5-turbo  # Model for context generation (use cheaper models to save cost)
LLM_TEMPERATURE=0.3  # Lower temperature for consistent context generation
LLM_MAX_TOKENS=150  # Max tokens for context generation (2-3 sentences)

# Knowledge Base Configuration
KB_TYPE=local  # Options: local, dify, raptor, graphrag
KB_LOCAL_BASE_URL=http://localhost:4242  # Local retrieval pipeline URL

# Chunking Configuration
CHUNK_SIZE=2048  # Characters per chunk
MAX_CHUNK_SIZE=4096  # Maximum chunk size
CHUNK_OVERLAP=200  # Overlap between chunks
RESPECT_PARAGRAPH_BOUNDARY=true  # Preserve paragraph structure

# Contextual Retrieval Settings
USE_CONTEXTUAL=true  # Enable contextual retrieval by default
ENABLE_COMPARISON=false  # Enable comparison mode for evaluation
CACHE_CONTEXTS=true  # Cache generated contexts to reduce API calls

# Agent Configuration
AGENT_MAX_ITERATIONS=10  # Max iterations for ReAct loop
AGENT_VERBOSE=true  # Enable detailed logging
CONVERSATION_HISTORY_LIMIT=20  # Number of messages to keep in history

# Performance Settings
BATCH_SIZE=10  # Number of chunks to process in parallel
MAX_WORKERS=4  # Maximum parallel workers for processing
CACHE_SIZE=1000  # Maximum number of contexts to cache

# Cost Control
MAX_CONTEXT_GENERATION_COST=10.0  # Maximum cost in USD for context generation
WARN_AT_COST=5.0  # Warn when cost exceeds this amount